===============
Metrics sources
===============

**This software is pre-production and should not be deployed to production servers.**

.. contents:: Table of Contents

Introduction
============

This documents describes briefly how metrics are collected, what is potential overhead and
how it can be enabled or disabled and how metrics are labeled.

Common labels
=============

Because all WCA metrics are collected in context of single physical machine (host), all metrics
will have ``host`` label attached representing the hostname::

    platform_topology_cpus{host="node37"} 40

There are also other common labels that can be enabled with ``MeasurementRunner``
in ``include_optional_labels`` boolean field:

- ``sockets`` - topology information: number of sockets	.. contents:: Table of Contents
- ``cores`` - topology information:  number of physical cores
- ``cpus`` - topology information: number of logical CPUs (threads)
- ``cpu_model`` - verbose name of processor of the host as presented by /proc/cpuinfo "model name"
- ``cpu_model_number`` -  Processor model of the host as presented by /proc/cpuinfo "model"
- ``wca_version`` - WCA build information e.g. 2.0.0a1-gxxx

Sources list
============

WCA collects metrics from multiple sources:

- **perf subsystem with cgroups** - hardware counters for cgroups using "perf_events" controller
- **perf subsystem with dynamic PMUs (uncore)** - hardware counters for platform using dynamic PMUs e.g. QPI/UPI usage,
- **resctrl filesystem** - metrics provided by "resctrl" for accessing Intel RDT metrics e.g. cache and memory bandwidth
- **cgroup fielsystem** - all metrics directly available in cgroup filesystem e.g. from cpuacct or memory controllers
- **/proc filesystem** - metrics exposed by kernel in "procfs"
- **/sys filesystem** - metrics exposed by kernel in "sysfs"
- **internal** - metrics generated by WCA itself for health checks or profiling
- **derived** - metrics calculated by WCA that use any above defined metrics
- **orchestrator**  - some date from Kubernetes or Mesos, e.g. number of requests CPUs
- ``dmidecode`` and ``ipmctl`` **binaries output** - to gather information about memory class hardware topology
- **/proc/PID/smaps** and **/proc/PID/clear_refs** - to try working set size of workload based of referenced pages,


Some of above metrics may have impact of performance for some applications and others have some limitations
e.g. number of hardware counters used at the same time.

All those metrics are configure by `MeasurementRunner object described in API <api.rst#measurementrunner>`_.

Perf subsystem with cgroups
-----------------------------

To collect those metrics you need to provide ``event_names`` list like:

.. code-block:: yaml

    !MeasurementRunner
    events_names:
    - task_cycles
    - task_instructions

There is also an options to configure any raw event by providing event_id and umask and countermask.

.. code-block:: yaml

    !MeasurementRunner
    events_names:
    - task_cycles
    - my_instruction__rC000
    - mem_inst_retired_all_loads__rD081
    - mem_inst_retired_all_stores__rD082

Following formats are supported (the ``__r`` is used to distinguish them from ordinary metrics):

- ``name__rEEUU``
- ``name__rEEUUCC``
- ``name__rEEUUCC1111111111``

where:

- ``EE`` is event number (Event select field), (byte)
- ``UU`` is umask (Unit Mask) (byte)
- ``CMASK`` is optional (Counter Mask) (byte)
- ``config1`` is optionl "config1" bytes (5bytes, 40bits)

EE,UU and CC are parsed as hex. Please refer to for exact meanings and values to "Intel Software Developer Manual Volume 2, Chapter 18.2.1".

Those metrics to be collected require task to be put in cgroup *perf_event* subsystem.

.. note::

   **Only a few or several hardware events can be collected at the same time, because
   Processor have a fixed number of registers which can be programmed to gain hardware information!
   WCA collects above events in transaction without multiplexing for better precision but cannot prevent
   other applications to use 'perf subsystem' (even other WCA instances). That may lead that
   all events will be scaled (estimated). The scaling factor can be observed but two dedicated
   metrics (enabled if any event based metrics are collected): ``task_scaling_factor_avg`` and
   ``task_scaling_factor_max``. Any values of "scaling" metrics above 1.0 indicate that "hardware
   counters" are shared and scaling was used to estimate real value.**

Perf subsystem with dynamic PMUs (uncore)
-----------------------------------------

Those metrics are collected from dynamic PMUs discovered and registered at boot time as described
by `man perf_event_open <http://man7.org/linux/man-pages/man2/perf_event_open.2.html>`_ in dynamic PMU section.

Only two types of PMU are currently support:
- iMC - for integrated memory controller
- UPI - for Ultra Path Interconnect

Those metrics are collect by per-platform basis.

Resctrl filesystem
-----------------------------

To collect metrics you need to have hardware with `Intel RDT <https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html>`_ support and set ``rdt_enabled`` in config file.

Collecting of this metrics can be controlled by "rdt_enabled" option.

"rdt_enable" option accepts three values:

- **None** (automatically) - collection of those metrics depends on hardware and kernel support for RDT
- **true** - resctrl based metrics are forced to be collected and WCA will stop with error if it is not possible,
- **false** - resctrl based metrics are not collected, even if RDT is available

Those metrics are collect by per-task basis.

Cgroup based
-----------------------------

Some metrics are collected directly from cgroup filesystem from specific controllers like cpu, cpuset
cpuacct or memory.

Those metrics cause minimal overhead so cannot be disabled.

Metrics are summed when there are multiple containers in a task.

/proc/ or /sys/ filesystems based
---------------------------------------

Those metrics are collected by reading form ``/proc`` (procfs) and ``/sys`` (sysfs) Linux filesystems.

Those metrics cause minimal overhead so cannot be disabled.

Internal
---------------------------------------

Those metrics are generated by WCA itself and cannot be disabled.

Derived
---------------------------------------

Those metrics are calculated by WCA by some equations hardcode in WCA code.

All those metrics can be enabled or disabled by ``enable_derived_metrics`` option.

Orchestrator
---------------------------------------

Those metrics are created based on metadata read from orchestrator e.g. Kubernetes or Mesos and cannot be disabled.


dmidecode and ipmctl
---------------------------------------

If available WCA tries to collect information about memory topology and configuration from those tools.
Similarly to ``resctrl`` **rdt_enabled** option are enabled by default (auto mode) but not required and will be provided if those data is available.

Is configured by ``gather_hw_mm_topology`` option:

- **None** (automatically) - collection of those metrics depends on required binaries availability - if not possible, metrics will not be generated,
- **true** - WCA requires those binaries and if not possible, stops with error,
- **false** - WCA will no try to collect those metrics at all

/proc/PIDS/smaps & /proc/PIDS/clear_refs
------------------------------------------

``TASK_WSS_REFERENCED_BYTES`` metrics tries to roughly estimate memory working set for workloads by reseting
"referenced bit" for every page for proceses and reading "Referenced" field from "smaps".

This metric collection is controller by **wss_reset_interval**. The **interval** is expressed in as number of WCA
iteration that WCA resets and reads values of referenced bytes.

